{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/fox/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from newspaper import build\n",
    "from newspaper import Article\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from datetime import datetime\n",
    "import schedule\n",
    "import time\n",
    "\n",
    "nltk.download(\"punkt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "I'm working...\n",
      "\n",
      "newspaper:  The Wall Street Journal\n",
      "\n",
      "article failed to download\n",
      "https://www.wsj.com/news/latest-headlines?mod=wsjheader\n",
      "\n",
      "article failed to download\n",
      "https://www.wsj.com/news/world?mod=nav_top_section\n",
      "\n",
      "article failed to download\n",
      "https://www.wsj.com/news/types/africa-news?mod=nav_top_subsection\n",
      "\n",
      "time:  2022-09-27 17:01:23.821510\n",
      "\n",
      "number of articles in the database:  0\n",
      "\n",
      "bad urls:\n",
      "\n",
      "newspaper:  The Financial Times\n",
      "\n",
      "title:  Financial Times\n",
      "url:  https://www.ft.com/companies/media\n",
      "\n",
      "title:  Bitcoin mining: Watt is money? | FT Standpoint\n",
      "url:  https://www.ft.com/video/4195bbed-8749-481e-a3a4-94815057df5f\n",
      "\n",
      "title:  Will Tesla's Optimus robot become a reality? | FT Tech\n",
      "url:  https://www.ft.com/video/51e4bfd4-9d50-43a7-afa1-53030dcf65fc\n",
      "\n",
      "time:  2022-09-27 17:01:29.985632\n",
      "\n",
      "number of articles in the database:  3\n",
      "\n",
      "bad urls:\n",
      "\n",
      "newspaper:  Le Monde\n",
      "\n",
      "title:  La livre sterling tombe à un plus-bas historique face au dollar\n",
      "url:  https://www.lemonde.fr/economie/article/2022/09/27/la-livre-sterling-tombe-a-un-plus-bas-historique_6143342_3234.html\n",
      "\n",
      "title:  De Netflix à Apple ou Amazon, une vague de nouveaux entrants dans la publicité\n",
      "url:  https://www.lemonde.fr/economie/article/2022/09/19/de-netflix-a-apple-ou-amazon-une-vague-de-nouveaux-entrants-dans-la-publicite_6142187_3234.html\n",
      "\n",
      "title:  Rachat de M6 : investir dans les médias « c’est un pari irrationnel, une forme de spectacle que l’on s’offre »\n",
      "url:  https://www.lemonde.fr/economie/article/2022/09/27/rachat-de-m6-investir-dans-les-medias-c-est-un-pari-irrationnel-une-forme-de-spectacle-que-l-on-s-offre_6143377_3234.html\n",
      "\n",
      "time:  2022-09-27 17:01:33.916693\n",
      "\n",
      "number of articles in the database:  6\n",
      "\n",
      "bad urls:\n",
      "\n",
      "newspaper:  The China Daily\n",
      "\n",
      "title:  我国将加大对新型基础设施建设的支持力度\n",
      "url:  https://language.chinadaily.com.cn/a/202209/27/WS6332a2a6a310fd2b29e7a04b.html\n",
      "\n",
      "title:  为减少食物浪费 日本酒店研制出“无边面包”\n",
      "url:  https://language.chinadaily.com.cn/a/202209/27/WS63324408a310fd2b29e79da3.html\n",
      "\n",
      "title:  日落西岳\n",
      "url:  https://language.chinadaily.com.cn/a/202209/27/WS63323d00a310fd2b29e79d8f.html\n",
      "\n",
      "time:  2022-09-27 17:01:48.572051\n",
      "\n",
      "number of articles in the database:  9\n",
      "\n",
      "bad urls:\n"
     ]
    }
   ],
   "source": [
    "fave_news = [\n",
    "    {\n",
    "        \"outlet\": \"The Wall Street Journal\",\n",
    "        \"url\": \"https://www.wsj.com/\",\n",
    "        \"slugs\": [\"blabla\"],\n",
    "        \"bad_urls\": [\"blabla\"],\n",
    "    },\n",
    "    {\n",
    "        \"outlet\": \"The Financial Times\",\n",
    "        \"url\": \"https://www.ft.com/\",\n",
    "        \"slugs\": [\"blabla\"],\n",
    "        \"bad_urls\": [\"blabla\"],\n",
    "    },\n",
    "    {\n",
    "        \"outlet\": \"Le Monde\",\n",
    "        \"url\": \"https://www.lemonde.fr/\",\n",
    "        \"slugs\": [\"blabla\"],\n",
    "        \"bad_urls\": [\"blabla\"],\n",
    "    },\n",
    "    {\n",
    "        \"outlet\": \"The China Daily\",\n",
    "        \"url\": \"https://www.chinadaily.com.cn/\",\n",
    "        \"slugs\": [\"blabla\"],\n",
    "        \"bad_urls\": [\"blabla\"],\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "date = datetime.today().strftime(\"%Y-%m-%d\")\n",
    "data = []\n",
    "urls_set = set()\n",
    "bad_urls = []\n",
    "\n",
    "\n",
    "def crawl():\n",
    "    print(\"\\nI'm working...\")\n",
    "\n",
    "    for item in fave_news:\n",
    "        news_outlet = item[\"outlet\"]\n",
    "        print(\"\\nnewspaper: \", news_outlet)\n",
    "        articles = []\n",
    "        paper_articles = build(\n",
    "            item[\"url\"], memoize_articles=False, fetch_images=False, MIN_WORD_COUNT=400\n",
    "        )\n",
    "        for article in paper_articles.articles:\n",
    "            # check to see if the article url doesn't already exist in the list\n",
    "            if article.url not in urls_set:\n",
    "                # and that the url is not in the bad_urls list\n",
    "                if article.url not in item[\"bad_urls\"]:\n",
    "                    # and that the url doesn't lead to a video or a comment section\n",
    "                    if any(slug in article.url for slug in item[\"slugs\"]):\n",
    "                        bad_urls.append(article.url)\n",
    "                    else:\n",
    "                        urls_set.add(article.url)\n",
    "                        articles.append(article.url)\n",
    "                else:\n",
    "                    bad_urls.append(article.url)\n",
    "            if len(articles) == 3:\n",
    "                break\n",
    "\n",
    "        ### Retrieve metadata and text for each article ###\n",
    "\n",
    "        for i in articles:\n",
    "            my_article = Article(i, language=\"en\")\n",
    "\n",
    "            try:\n",
    "                my_article.download()\n",
    "                my_article.parse()\n",
    "                my_article.nlp()\n",
    "            except:\n",
    "                print(\"\\narticle failed to download\")\n",
    "                print(i)\n",
    "                continue\n",
    "\n",
    "            url = i\n",
    "            title = my_article.title\n",
    "            text = TextBlob(my_article.text)\n",
    "            keywords = my_article.keywords\n",
    "\n",
    "            \"\"\"\n",
    "            Subjectivity is the output that lies within [0,1] and refers to personal opinions and judgments.\n",
    "            Polarity is the output that lies between [-1,1], where -1 refers to negative sentiment and +1 refers to positive sentiment.\n",
    "            \"\"\"\n",
    "            polarity = text.sentiment.polarity\n",
    "            subjectivity = text.sentiment.subjectivity\n",
    "\n",
    "            print(\"\\ntitle: \", title)\n",
    "            print(\"url: \", url)\n",
    "            # print(\"\\nkeywords: \", keywords)\n",
    "            # print(\"\\npolarity: \", polarity)\n",
    "            # print(\"\\nsubjectivity: \", subjectivity)\n",
    "            # print(\"\\nsummary: \", my_article.summary)\n",
    "\n",
    "            data.append(\n",
    "                [date, news_outlet, url, title, text, keywords, polarity, subjectivity]\n",
    "            )\n",
    "\n",
    "        print(\"\\ntime: \", datetime.now())\n",
    "        print(\"\\nnumber of articles in the database: \", len(data))\n",
    "        print(\"\\nbad urls:\")\n",
    "        for i in bad_urls:\n",
    "            print(i)\n",
    "    return\n",
    "\n",
    "\n",
    "# schedule.every().day.at(\"21:00\").do(crawl)\n",
    "# schedule.every(1).minutes.do(crawl)\n",
    "\n",
    "# while True:\n",
    "#     schedule.run_pending()\n",
    "#     time.sleep(60)  # wait one minute\n",
    "\n",
    "crawl()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The China Daily\n",
      "日落西岳\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# [date, news_outlet, url, title, text, keywords, polarity, subjectivity]\n",
    "i = 8\n",
    "print(data[i][1])\n",
    "print(data[i][3])\n",
    "print(data[i][4])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
